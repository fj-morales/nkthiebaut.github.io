{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot learning on textual data with siamese neural networks\n",
    "\n",
    "If you're doing machine learning and meet a classification problem with many categories and few examples per category, it is usually thought that you're in trouble üò®. Acquiring new data to solve this issue is not always easy or even doable. Luckily, we'll see that efficient techniques exist to deal with this situation üï∫. \n",
    "\n",
    "This problem of learning with only a few examples per category is called \"few-shot learning\", and \"one-shot learning\" in the extreme case of only one example per class (yes, you can even do this and [obtain decent results](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)!). \n",
    "\n",
    "Most of the machine learning research on one-shot learning involves images, but some [recent research papers](https://arxiv.org/abs/1710.10280) address the same problem in the Natural Language Processing (NLP) realm.\n",
    "\n",
    "In this blog post, I will use siamese neural network to tackle few-shot learning, following a [method that was originally applied to images](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) and that is nicely explained [here](https://sorenbouma.github.io/blog/oneshot/).\n",
    "\n",
    "Job title classification provides a good example of a few-shot learning problem in NLP. Say you want to group job titles in different categories or \"occupations\" (e.g. gather \"Programmer\" and \"Software engineer\" in an occupation, and \"Sales manager\" in another one), then unless you have hundreds of job titles examples per occupation you are facing a few-shot learning problem. The U.S government provides such a job title/occupations taxonomy: the [Standard Occupational Classification](https://www.bls.gov/soc/). I'll use it as a toy dataset understand how few-shot learning with siamese neural networks works.\n",
    "\n",
    "Let's start by downloading the taxonomy and check what's in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reported Job Title</th>\n",
       "      <th>SOC minor group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chief executive officer (ceo)</td>\n",
       "      <td>Top Executives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chief financial officer (cfo)</td>\n",
       "      <td>Top Executives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chief nursing officer</td>\n",
       "      <td>Top Executives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>banking center manager (bcm)</td>\n",
       "      <td>Operations Specialties Managers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>banking officer</td>\n",
       "      <td>Operations Specialties Managers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>branch manager</td>\n",
       "      <td>Operations Specialties Managers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>project engineering manager</td>\n",
       "      <td>Other Management Occupations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>project manager</td>\n",
       "      <td>Other Management Occupations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>analytical research program manager</td>\n",
       "      <td>Other Management Occupations</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Reported Job Title                  SOC minor group\n",
       "1          chief executive officer (ceo)                   Top Executives\n",
       "2          chief financial officer (cfo)                   Top Executives\n",
       "3                  chief nursing officer                   Top Executives\n",
       "100         banking center manager (bcm)  Operations Specialties Managers\n",
       "101                      banking officer  Operations Specialties Managers\n",
       "102                       branch manager  Operations Specialties Managers\n",
       "301          project engineering manager     Other Management Occupations\n",
       "302                      project manager     Other Management Occupations\n",
       "303  analytical research program manager     Other Management Occupations"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Import a home-made dictionary that maps SOC codes to \"occupations\" names\n",
    "from soc import SOC_MINOR_GROUPS\n",
    "\n",
    "# Download the Standard Occupation Classification with job titles examples\n",
    "file_url = 'https://www.onetcenter.org/dl_files/database/db_20_1_text/Sample%20of%20Reported%20Titles.txt'\n",
    "csv = StringIO(requests.get(file_url).text)\n",
    "\n",
    "# Load it in a pandas DataFrame and drop a useless column\n",
    "df = pd.read_csv(csv, sep='\\t').drop('Shown in My Next Move', axis=1)\n",
    "\n",
    "# Get the occupation name from the code and remove the original code column\n",
    "df['SOC minor group'] = df['O*NET-SOC Code'].apply(lambda x: SOC_MINOR_GROUPS[x[:4]])\n",
    "df.drop('O*NET-SOC Code', axis=1, inplace=True)\n",
    "\n",
    "# Lower all job titles for simplicity\n",
    "df['Reported Job Title'] = df['Reported Job Title'].str.lower()\n",
    "\n",
    "# Display a few examples\n",
    "df.iloc[[1,2,3,100,101,102,301,302,303]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded file contains job categories codes (\"SOC minor group\") and samples of job titles that belong to those categories. To get categories description check [the Standard Occupation Classification website](https://www.bls.gov/soc/2010/2010_major_groups.htm).\n",
    "\n",
    "Let's investigate a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reported Job Title    7174\n",
       "SOC minor group         94\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()  # Count the number of different modalities in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 94 categories (\"SOC minor groups\") for 7174 examples, i.e. 75 examples per category on average, but some categories have as few as 10 examples (a df.value_counts() would tell you that). This is not the most extreme examples of few-shot learning but it's still an example that is better tacked by the siamese techniques that will see below than with a standard multi-class classification approach. \n",
    "\n",
    "Before proceeding with modelling let's create a train and test sets, by putting one example in the test set for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = df.groupby('SOC minor group', as_index=False)['Reported Job Title'].first()\n",
    "train_set = df[~df['Reported Job Title'].isin(test_set['Reported Job Title'])]\n",
    "\n",
    "x_train, y_train = train_set['Reported Job Title'], train_set['SOC minor group']\n",
    "x_test, y_test = test_set['Reported Job Title'], test_set['SOC minor group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a baseline üê£\n",
    "\n",
    "Before experimenting with fancy models, let's establish a strong baseline. We can start by using word embeddings to get a vector representation of each job title, and use a nearest neighbor classifier that is a less likely to overfit than tree-based models or parametric classifiers.\n",
    "\n",
    "To get the representation of a sentence from pre-trained word embeddings I'll use [Zeugma](https://github.com/nkthiebaut/zeugma), an NLP python library I've written that conveniently provides pre-trained word embeddings in the form of [scikit-learn transformers](http://scikit-learn.org/stable/modules/pipeline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeugma import EmbeddingTransformer\n",
    "\n",
    "# We'll use the GloVe pre-trained embeddings, using the sum of the word embeddings\n",
    "# of a job title as the embedding vector\n",
    "embedding = EmbeddingTransformer('glove-twitter-200', aggregation='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Our model is a nearest neighbor classifier, the input of which is the sum of the \n",
    "# embeddings of words in the job title.\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "baseline = make_pipeline(embedding, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy (baseline): 73.81 %\n",
      "Test accuracy (baseline): 50.00 %\n"
     ]
    }
   ],
   "source": [
    "baseline.fit(x_train, y_train)\n",
    "print(f'Train accuracy (baseline): {100*baseline.score(x_train, y_train):.2f} %')\n",
    "print(f'Test accuracy (baseline): {100*baseline.score(x_test, y_test):.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "y_pred = baseline.predict(x_test)\n",
    "y_prob = baseline.predict_proba(x_test)\n",
    "# def roc_auc_multiclass(y_test, y_pred, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9385419861076632"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soc_minor_groups = np.unique(y_pred)\n",
    "roc_auc_score(\n",
    "    label_binarize(y_test, classes=np.unique(y_test)), \n",
    "    y_prob,\n",
    "    average='micro'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely not great but not too bad for a simple baseline model, considering a random guess would have a $\\frac{1}{n_{\\text{classes}}} \\simeq 0.1 \\%$ accuracy. \n",
    "\n",
    "It may seem hard to beat this simple baseline with a deep learning model due to the high chances of overfitting with such a small dataset, but here come siamese networks to the rescue.\n",
    "\n",
    "\n",
    "## Few-shot learning with siamese neural networks üëØ‚Äç‚ôÄÔ∏è\n",
    "\n",
    "The nearest neighbor model of the previous section is preforming quite well despite its simplicity, because it uses word embeddings learnt on a [huge NLP dataset from Twitter](https://nlp.stanford.edu/projects/glove/). Using word representations learnt in an unsupervised setting on a bigger dataset is the basic principle of [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) and reduces the overfit by allowing smaller models to perform well, utltimately giving better performances on the test set.\n",
    "\n",
    "Although the pre-trained embeddings are valuable, the embedding space used to determine nearest neighbors knows nothing about job titles in particular. There must be a way to learn an embedding space in which jobs belonging to the same occupation category are closer. This is where siamese networks come into play.\n",
    "\n",
    "The main idea of siamese networks is to learn such a representation by training a model to discriminate between pairs of examples that are in the same category, and pairs of examples that come from different categories. \n",
    "\n",
    "\n",
    "### Building the pairs dataset\n",
    "\n",
    "Let's create positive samples with pairs of job titles corresponding to the same SOC, and negative examples with pairs of job titles sampled from different SOC codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_left</th>\n",
       "      <th>job_right</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96427</th>\n",
       "      <td>staff genetic counselor</td>\n",
       "      <td>certified professional midwife</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213759</th>\n",
       "      <td>double end tenon operator</td>\n",
       "      <td>fitness instructor</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115334</th>\n",
       "      <td>certified executive chef (cec)</td>\n",
       "      <td>cost analyst</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192141</th>\n",
       "      <td>lubricator</td>\n",
       "      <td>trades helper</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182783</th>\n",
       "      <td>repair technician</td>\n",
       "      <td>substation mechanic</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              job_left                       job_right  target\n",
       "96427          staff genetic counselor  certified professional midwife     1.0\n",
       "213759       double end tenon operator              fitness instructor     0.0\n",
       "115334  certified executive chef (cec)                    cost analyst     0.0\n",
       "192141                      lubricator                   trades helper     1.0\n",
       "182783               repair technician             substation mechanic     1.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "from random import sample\n",
    "\n",
    "jobs_left = []\n",
    "jobs_right = []\n",
    "target = []\n",
    "\n",
    "soc_codes = train_set['SOC minor group'].unique()\n",
    "for code in soc_codes:\n",
    "    similar_jobs = train_set[train_set['SOC minor group'] == code]['Reported Job Title']\n",
    "    # Pick 1000 random pairs from the SOC group's job titles combinations \n",
    "    group_pairs = list(itertools.combinations(similar_jobs, 2)) \n",
    "    positive_pairs = sample(group_pairs, 2000) if len(group_pairs) > 2000 else group_pairs\n",
    "    jobs_left.extend([p[0] for p in positive_pairs])\n",
    "    jobs_right.extend([p[1] for p in positive_pairs])\n",
    "    target.extend([1.]*len(positive_pairs))\n",
    "    \n",
    "    other_jobs = df[df['SOC minor group'] != code]['Reported Job Title']\n",
    "    for i in range(len(positive_pairs)):\n",
    "        jobs_left.append(np.random.choice(similar_jobs))\n",
    "        jobs_right.append(np.random.choice(other_jobs))\n",
    "        target.append(0.)\n",
    "\n",
    "dataset = pd.DataFrame({\n",
    "        'job_left': jobs_left,\n",
    "        'job_right': jobs_right,\n",
    "        'target': target\n",
    "    }).sample(frac=1)  # Shuffle dataset\n",
    "\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the pair \"professor\" and \"spanish instructor\" belong to the same job occupation category, while \"sports writer\" and \"animal scientist\" are in different occupations. Note that we end up with a much bigger dataset by creating pairs. Indeed the number of the created siamese dataset has 218,669 while the original dataset has only 8,921 samples. Of course we have only artifically increased the dataset size because we have not generated new data, but we'll see that this technique is still very powerful.\n",
    "\n",
    "\n",
    "### Modelling\n",
    "\n",
    "The general architecture of the model is based on [this very good tutorial](https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07). I first lighly preprocess the job titles, turn them into index sequences and pad them to get a valide input for the incoming neural network classifier. Once again I use convenient transformers from the [Zeugma](https://github.com/nkthiebaut/zeugma) to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.pipeline import make_pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from zeugma import TextsToSequences, Padder, ItemSelector\n",
    "\n",
    "max_words_job_title = 10  # To avoid very long job titles we limit them to 10 words\n",
    "vocab_size = 10000  # Number of most-frequent words kept in the vocabulary\n",
    "\n",
    "def preprocess_job_titles(job_titles):\n",
    "    \"\"\" Return a list of clean job titles \"\"\"\n",
    "    def preprocess_job_title(raw_job_title):\n",
    "        \"\"\" Clean a single job title\"\"\"\n",
    "        job_title = re.sub(r'\\(.*\\)', '', raw_job_title)  # Remove everything between parenthesis\n",
    "        return job_title.lower().strip()\n",
    "    return [preprocess_job_title(jt) for jt in job_titles]\n",
    "    \n",
    "pipeline = make_pipeline(\n",
    "    FunctionTransformer(preprocess_job_titles, validate=False),  # Preprocess the text\n",
    "    TextsToSequences(num_words=vocab_size),  # Turn word sequences into indexes sequences\n",
    "    Padder(max_length=max_words_job_title),  # Pad shorter job titles with a dummy index\n",
    ")\n",
    "\n",
    "# Note that the preprocessing pipeline must be fit on both the right and left examples\n",
    "# simultaneously\n",
    "pipeline.fit(list(dataset['job_left']) + list(dataset['job_right']));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_left = pipeline.transform(dataset['job_left'])\n",
    "x_right = pipeline.transform(dataset['job_right'])\n",
    "x_pairs = [x_left, x_right]   # this will be the input of the siamese network\n",
    "\n",
    "y_pairs = dataset['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-use the same embedding as with the baseline model\n",
    "embedding_layer = embedding.model.get_keras_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the pairs dataset and preprocessed the job titles we can turn to the siamese model itself. It consists of a cloned sequential network, the input of which is a pair of vectors `x_left` and `x_right`. The last layer of the sequential network for `x_left` is the vector representation of the left job title, and same thing for `x_right` the right input job title. The representations of the right and left inputs are used to compute the similarity between the job titles:\n",
    "\n",
    "$$\\text{sim}\\left(x_{l}, x_{r}\\right) = \\exp \\left(-\\| f(x_l) - f(x_r) \\|_1\\right),$$\n",
    "\n",
    "where $\\text{sim} \\in [0, 1]$, $\\|\\cdot\\|_1$ is the L1 norm, and $f$ is the function corresponding to the application of the cloned sequential network to the left/right input. \n",
    "\n",
    "This setting is called the [Manhattan LSTM](https://www.quora.com/What-is-Manhattan-LSTM) becasue the we'll use LSTMs as the sequential network, and the L1 norm used to compute the distance between two samples of a pair is also called the [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry). Here is the corresponding code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "left_input (InputLayer)         (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_input (InputLayer)        (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_network (Sequential) (None, 64)           238879696   left_input[0][0]                 \n",
      "                                                                 right_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pair_representations_difference (None, 64)           0           sequential_network[1][0]         \n",
      "                                                                 sequential_network[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "masltsm_distance (Lambda)       (None, 1)            0           pair_representations_difference[0\n",
      "==================================================================================================\n",
      "Total params: 238,879,696\n",
      "Trainable params: 176,896\n",
      "Non-trainable params: 238,702,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, Subtract\n",
    "from keras import backend as K\n",
    "\n",
    "def exponent_neg_manhattan_distance(arms_difference):\n",
    "    \"\"\" Compute the exponent of the opposite of the L1 norm of a vector, to get the left/right inputs\n",
    "    similarity from the inputs differences. This function is used to turned the unbounded\n",
    "    L1 distance to a similarity measure between 0 and 1\"\"\"\n",
    "    return K.exp(-K.sum(K.abs(arms_difference), axis=1, keepdims=True))\n",
    "\n",
    "def siamese_lstm(max_length=max_words_job_title, embedding_layer=embedding_layer):\n",
    "    \"\"\" Define, compile and return a siamese LSTM model \"\"\"\n",
    "    input_shape = (max_length,)\n",
    "    left_input = Input(input_shape, name='left_input')\n",
    "    right_input = Input(input_shape, name='right_input')\n",
    "\n",
    "    # Define a single sequential model for both arms.\n",
    "    # In this example I've chosen a stack of 2 bidirectional LSTMs with a bit of dropout\n",
    "    seq = Sequential(name='sequential_network')\n",
    "    seq.add(embedding_layer)\n",
    "    seq.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "    seq.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    \n",
    "    left_output = seq(left_input)\n",
    "    right_output = seq(right_input)\n",
    "\n",
    "    # Here we subtract the neuron values of the last layer from the left arm \n",
    "    # with the corresponding values from the right arm\n",
    "    subtracted = Subtract(name='pair_representations_difference')([left_output, right_output])\n",
    "    malstm_distance = Lambda(exponent_neg_manhattan_distance, \n",
    "                             name='masltsm_distance')(subtracted)\n",
    "\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=malstm_distance)\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "    return siamese_net\n",
    "\n",
    "siamese_lstm = siamese_lstm()\n",
    "\n",
    "# Print a summary of the model mainly to know the number of trainable parameters\n",
    "siamese_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 218669 samples, validate on 24297 samples\n",
      "Epoch 1/10\n",
      "218669/218669 [==============================] - 249s 1ms/step - loss: 0.6325 - acc: 0.6406 - val_loss: 0.5472 - val_acc: 0.7231\n",
      "Epoch 2/10\n",
      "218669/218669 [==============================] - 239s 1ms/step - loss: 0.5323 - acc: 0.7422 - val_loss: 0.4771 - val_acc: 0.7882\n",
      "Epoch 3/10\n",
      "218669/218669 [==============================] - 2723s 12ms/step - loss: 0.4865 - acc: 0.7827 - val_loss: 0.4358 - val_acc: 0.8190\n",
      "Epoch 4/10\n",
      "218669/218669 [==============================] - 553s 3ms/step - loss: 0.4578 - acc: 0.8041 - val_loss: 0.4092 - val_acc: 0.8392\n",
      "Epoch 5/10\n",
      "218669/218669 [==============================] - 552s 3ms/step - loss: 0.4371 - acc: 0.8202 - val_loss: 0.3863 - val_acc: 0.8522\n",
      "Epoch 6/10\n",
      "218669/218669 [==============================] - 436s 2ms/step - loss: 0.4207 - acc: 0.8302 - val_loss: 0.3710 - val_acc: 0.8585\n",
      "Epoch 7/10\n",
      "218669/218669 [==============================] - 250s 1ms/step - loss: 0.4073 - acc: 0.8375 - val_loss: 0.3565 - val_acc: 0.8659\n",
      "Epoch 8/10\n",
      "218669/218669 [==============================] - 244s 1ms/step - loss: 0.3969 - acc: 0.8428 - val_loss: 0.3475 - val_acc: 0.8682\n",
      "Epoch 9/10\n",
      "218669/218669 [==============================] - 246s 1ms/step - loss: 0.3863 - acc: 0.8474 - val_loss: 0.3350 - val_acc: 0.8735\n",
      "Epoch 10/10\n",
      "218669/218669 [==============================] - 221s 1ms/step - loss: 0.3779 - acc: 0.8502 - val_loss: 0.3326 - val_acc: 0.8729\n"
     ]
    }
   ],
   "source": [
    "siamese_lstm.fit(x_pairs, y_pairs, validation_split=0.1, epochs=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without much effort (light preprocessing, few epochs, no early stopping, no hyper-parameters optimization) we obtain a decent ~87 % accuracy on the validation set. But remember that this is not the final task, here we are only solving the binary classification problem of recognizing pairs of job titles that belong to the same occupations category and pairs of jobs that are sampled from different occupations categories.\n",
    "\n",
    "To address the initial problem of finding each job title's category we have to compute, for each example in the test set, the similarity score of this example with all the examples in the training set. The predicted category is the one of the closest example in training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_references = pipeline.transform(x_train)  # Preprocess the training set examples\n",
    "\n",
    "def get_prediction(job_title):\n",
    "    \"\"\" Get the predicted job title category, and the most similar job title\n",
    "    in the train set. Note that this way of computing a prediction is highly \n",
    "    not optimal, but it'll be sufficient for us now. \"\"\"\n",
    "    x = pipeline.transform([job_title])\n",
    "    # Compute similarities of the job title with all job titles in the train set\n",
    "    similarities = siamese_lstm.predict([[x[0]]*len(x_references), x_references])\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "    # The predicted category is the one of the most similar example from the train set\n",
    "    prediction = train_set['SOC minor group'].iloc[most_similar_index]\n",
    "    most_similar_example = train_set['Reported Job Title'].iloc[most_similar_index]\n",
    "    return prediction, most_similar_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 1\n",
    "pred, most_sim = get_prediction(x_test[sample_idx])\n",
    "print(f'Sampled test job title: {x_test[sample_idx]}')\n",
    "print(f'True occupation: {test_set[\"SOC minor group\"].iloc[sample_idx]}')\n",
    "print(f'Occupation prediction: {pred}')\n",
    "print(f'Most similar example in train set: {most_sim}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (siamese model): 38.00 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = [get_prediction(job_title)[0] for job_title in test_set['Reported Job Title'].iloc[:50]]\n",
    "print(f'Test accuracy (siamese model): {100*accuracy_score(y_pred, y_test[:50]):.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The siamese model thus outperforms the random guess (accuracy ~0.1 %) and the nearest neighbor baseline (~32 %) by a substantial margin ü§π‚Äç‚ôÄÔ∏è . Even though it is far from perfect, predicts the right category for a job title 2 times out of 5, while it has to choose between roughly a hundred of them. \n",
    "\n",
    "What this means is that the siamese model managed to squeeze some juice out of **all** the examples in the dataset, even accross categories. The original multi-class classification approach does not allow to learn \"accross categories\" because the categorical cross-entropy that is always used to treat those problems actually treats the multi-class classification tasks as a set of independent binary classification tasks. \n",
    "\n",
    "The siamese network approach to the few-shot learning problem is definitely a \n",
    "way out with textual data üìö. At the cost of a bit more complex modelling it gives better than standards multi-class classification methods. Give it a shot if you and to classify your data with machine learning but don't have many examples per category, you won't regret it üéä. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
