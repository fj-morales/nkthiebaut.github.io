{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're doing machine learning and meet a classification problem with many categories and only a few examples per category, it is usually thought that you're in trouble üò®. Acquiring new data to solve this issue is not always easy or even doable. Luckily, we'll see that efficient techniques exist to deal with this situation with Siamese Neural Networks üï∫. \n",
    "\n",
    "This problem of learning with only a few examples per category is called \"few-shot learning\", and \"one-shot learning\" in the extreme case of only one example per class (yes, you can even do this and [obtain decent results](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)!). \n",
    "\n",
    "Most of the machine learning research on one-shot learning involves images, but some [recent research papers](https://arxiv.org/abs/1710.10280) address the same problem in the Natural Language Processing (NLP) realm.\n",
    "\n",
    "In this blog post, I will use a siamese neural network to tackle a few-shot learning prolbem, following a [method that was originally applied to images](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) and that is nicely explained [here](https://sorenbouma.github.io/blog/oneshot/).\n",
    "\n",
    "Job title classification provides a good example of a few-shot learning problem in NLP. Imagine you want to group job titles in different categories or \"occupations\" (e.g. gather \"Programmer\" and \"Software engineer\" under the same occupation, and \"Sales manager\" and \"Account executive\" under another one). Unless you have hundreds of job titles examples per occupation you are facing a few-shot learning problem. The U.S government provides such a job title/occupations taxonomy: the [Standard Occupational Classification](https://www.bls.gov/soc/). I'll use it as a toy dataset to understand how few-shot learning with siamese neural networks works.\n",
    "\n",
    "Let's start by downloading the taxonomy and check what's in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reported Job Title</th>\n",
       "      <th>SOC minor group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chief executive officer (ceo)</td>\n",
       "      <td>Top Executives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chief financial officer (cfo)</td>\n",
       "      <td>Top Executives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chief nursing officer</td>\n",
       "      <td>Top Executives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>banking center manager (bcm)</td>\n",
       "      <td>Operations Specialties Managers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>banking officer</td>\n",
       "      <td>Operations Specialties Managers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>branch manager</td>\n",
       "      <td>Operations Specialties Managers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>project engineering manager</td>\n",
       "      <td>Other Management Occupations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>project manager</td>\n",
       "      <td>Other Management Occupations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>analytical research program manager</td>\n",
       "      <td>Other Management Occupations</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Reported Job Title                  SOC minor group\n",
       "1          chief executive officer (ceo)                   Top Executives\n",
       "2          chief financial officer (cfo)                   Top Executives\n",
       "3                  chief nursing officer                   Top Executives\n",
       "100         banking center manager (bcm)  Operations Specialties Managers\n",
       "101                      banking officer  Operations Specialties Managers\n",
       "102                       branch manager  Operations Specialties Managers\n",
       "301          project engineering manager     Other Management Occupations\n",
       "302                      project manager     Other Management Occupations\n",
       "303  analytical research program manager     Other Management Occupations"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "from random import seed\n",
    "import numpy as np\n",
    "seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import a home-made dictionary that maps SOC codes to \"occupations\" names\n",
    "# The corresponding file can be downloaded here: https://gist.github.com/nkthiebaut/c2895b8bb77bdf3253fb581622dca51b\n",
    "from soc import SOC_MINOR_GROUPS\n",
    "\n",
    "# Download the Standard Occupation Classification with job titles examples\n",
    "file_url = 'https://www.onetcenter.org/dl_files/database/db_20_1_text/Sample%20of%20Reported%20Titles.txt'\n",
    "csv = StringIO(requests.get(file_url).text)\n",
    "\n",
    "# Load it in a pandas DataFrame and drop a useless column\n",
    "df = pd.read_csv(csv, sep='\\t').drop('Shown in My Next Move', axis=1)\n",
    "\n",
    "# Get the occupation name from the code and remove the original code column\n",
    "df['SOC minor group'] = df['O*NET-SOC Code'].apply(lambda x: SOC_MINOR_GROUPS[x[:4]])\n",
    "df.drop('O*NET-SOC Code', axis=1, inplace=True)\n",
    "\n",
    "# Lower all job titles for simplicity\n",
    "df['Reported Job Title'] = df['Reported Job Title'].str.lower()\n",
    "\n",
    "# Display a few examples\n",
    "df.iloc[[1,2,3,100,101,102,301,302,303]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded file contains job categories codes (\"SOC minor group\") and samples of job titles that belong to those categories. The categories descriptions are available on [the Standard Occupation Classification website](https://www.bls.gov/soc/2010/2010_major_groups.htm).\n",
    "\n",
    "Let's investigate our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reported Job Title    7174\n",
       "SOC minor group         94\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()  # Count the number of different modalities in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 94 categories (\"SOC minor groups\") for 7174 examples, i.e. 75 examples per category on average, but some categories have as few as 10 examples (`df.value_counts()` would tell you that). This is not the most extreme examples of few-shot learning but it's still an example that is better tackled by siamese models than by standard multi-class classification approaches. \n",
    "\n",
    "Before proceeding with modelling let's create a train and test sets, by putting one example in the test set for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = df.groupby('SOC minor group', as_index=False)['Reported Job Title'].first()\n",
    "train_set = df[~df['Reported Job Title'].isin(test_set['Reported Job Title'])]\n",
    "\n",
    "x_train, y_train = train_set['Reported Job Title'], train_set['SOC minor group']\n",
    "x_test, y_test = test_set['Reported Job Title'], test_set['SOC minor group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then encode the targets as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "classes_encoder = LabelEncoder()\n",
    "\n",
    "y_train = classes_encoder.fit_transform(y_train)\n",
    "y_test = classes_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a baseline üê£\n",
    "\n",
    "Before experimenting with fancy models, let's establish a strong baseline. We can start by using word embeddings to get a vector representation of each job title and use a nearest neighbor classifier that is less likely to overfit than tree-based models or parametric classifiers.\n",
    "\n",
    "To get the representation of a sentence from pre-trained word embeddings I'll use [Zeugma](https://github.com/nkthiebaut/zeugma), an NLP python library I've written that conveniently provides pre-trained word embeddings in the form of [scikit-learn transformers](http://scikit-learn.org/stable/modules/pipeline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from zeugma import EmbeddingTransformer\n",
    "\n",
    "# We'll use the GloVe pre-trained embeddings, using the sum of the word embeddings\n",
    "# of a job title as the embedding vector\n",
    "embedding = EmbeddingTransformer('glove', aggregation='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Our model is a nearest neighbor classifier, the input of which is the sum of the \n",
    "# embeddings of words in the job title.\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "baseline = make_pipeline(embedding, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy (baseline): 84.94 %\n",
      "Test accuracy (baseline): 25.53 %\n"
     ]
    }
   ],
   "source": [
    "baseline.fit(x_train, y_train)\n",
    "print(f'Train accuracy (baseline): {100*baseline.score(x_train, y_train):.2f} %')\n",
    "print(f'Test accuracy (baseline): {100*baseline.score(x_test, y_test):.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great... but not too bad for a simple baseline model, considering a random guess would have a $\\frac{1}{n_{\\text{classes}}} \\simeq 0.1 \\%$ accuracy. I have tried a few other simple models but, surprisingly, I could not beat this simple baseline easily. Let me know if you find a better model!\n",
    "\n",
    "Note that the test accuracy is usually expected to be as high as the train one for a nearest neighbors classifier, since no parameters are fitted. Here this is not the case because the test set contains one example per category, such that rare categories have the same weight as prominent categories in the test accuracy.\n",
    "\n",
    "It may seem hard to beat this simple baseline with a deep learning model due to the high chances of overfitting with such a small dataset, but here come siamese networks to the rescue.\n",
    "\n",
    "\n",
    "## Few-shot learning with siamese neural networks üëØ‚Äç‚ôÄÔ∏è\n",
    "\n",
    "The nearest neighbor model of the previous section is performing quite well despite its simplicity because it uses word embeddings learned on a [huge NLP dataset from Twitter](https://nlp.stanford.edu/projects/glove/). Using these words embeddings is a basic form of [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning), which reduces the overfit by allowing smaller models to perform well, ultimately giving better performances on the test set.\n",
    "\n",
    "Although the pre-trained embeddings are valuable, the embedding space used to determine nearest neighbors knows nothing about job titles in particular. There must be a way to learn an embedding space in which jobs that belong to the same occupation category are closer. This is where siamese networks come into play.\n",
    "\n",
    "The main idea of siamese networks is to learn the above mentioned vector representation by training a model that discriminates between pairs of examples that are in the same category, and pairs of examples that come from different categories. \n",
    "\n",
    "\n",
    "### Building the pairs dataset\n",
    "\n",
    "Let's create positive samples with pairs of job titles from the same SOC category, and negative examples with pairs of job titles sampled from different SOC codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_left</th>\n",
       "      <th>job_right</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116767</th>\n",
       "      <td>carman</td>\n",
       "      <td>marine design engineer</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81897</th>\n",
       "      <td>childcare worker</td>\n",
       "      <td>recreation supervisor</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100046</th>\n",
       "      <td>brood hatchery manager</td>\n",
       "      <td>sow farm manager</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64558</th>\n",
       "      <td>public safety officer</td>\n",
       "      <td>traffic control officer</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14386</th>\n",
       "      <td>senior planner</td>\n",
       "      <td>compiler</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      job_left                job_right  target\n",
       "116767                  carman   marine design engineer     0.0\n",
       "81897         childcare worker    recreation supervisor     1.0\n",
       "100046  brood hatchery manager         sow farm manager     1.0\n",
       "64558    public safety officer  traffic control officer     1.0\n",
       "14386           senior planner                 compiler     1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "from random import sample\n",
    "\n",
    "jobs_left = []\n",
    "jobs_right = []\n",
    "target = []\n",
    "\n",
    "soc_codes = train_set['SOC minor group'].unique()\n",
    "for code in soc_codes:\n",
    "    # 1) create similar categories pairs, with a corresponding target of 1\n",
    "    similar_jobs = train_set[train_set['SOC minor group'] == code]['Reported Job Title']\n",
    "    \n",
    "    # Pick 1000 random pairs from the SOC group's job titles combinations \n",
    "    group_pairs = list(itertools.combinations(similar_jobs, 2)) \n",
    "    positive_pairs = sample(group_pairs, 1000) if len(group_pairs) > 1000 else group_pairs\n",
    "    jobs_left.extend([p[0] for p in positive_pairs])\n",
    "    jobs_right.extend([p[1] for p in positive_pairs])\n",
    "    target.extend([1.]*len(positive_pairs))\n",
    "    \n",
    "    # 2) create pairs of examples with jobs from different categories, with a target set to 0\n",
    "    other_jobs = train_set[train_set['SOC minor group'] != code]['Reported Job Title']\n",
    "    for i in range(len(positive_pairs)):\n",
    "        jobs_left.append(np.random.choice(similar_jobs))\n",
    "        jobs_right.append(np.random.choice(other_jobs))\n",
    "        target.append(0.)\n",
    "\n",
    "dataset = pd.DataFrame({\n",
    "        'job_left': jobs_left,\n",
    "        'job_right': jobs_right,\n",
    "        'target': target\n",
    "    }).sample(frac=1)  # Shuffle dataset\n",
    "\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the pair \"childcare worker\" and \"recreation supervisor\" belong to the same job occupation category (second example, target = 1), while \"carman\" and \"marine design engineer\" are in different occupation categories (first example, target = 0). Note that we end up with a much bigger dataset by creating pairs. The synthetic dataset contains 218,669 pairs while the original dataset has only 8,921 samples. Of course, we have only artificially increased the dataset size because we have not generated new data, but we'll see that this technique is still very powerful.\n",
    "\n",
    "\n",
    "### Modelling\n",
    "\n",
    "The general architecture of the model is based on [this very good tutorial](https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07). The preprocessing is fairly simple: \n",
    "- we remove parts of the jobs titles that are between parenthesis, we lowercase them, \n",
    "- then we turn the examples into index sequences, and\n",
    "- \"pad\" them to get a valid input for the neural network classifier. \n",
    "\n",
    "Here, again, I use convenient transformers from the [Zeugma](https://github.com/nkthiebaut/zeugma) to perform all those steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.pipeline import make_pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from zeugma import TextsToSequences, Padder, ItemSelector\n",
    "\n",
    "max_words_job_title = 10  # To avoid very long job titles we limit them to 10 words\n",
    "vocab_size = 10000  # Number of most-frequent words kept in the vocabulary\n",
    "\n",
    "def preprocess_job_titles(job_titles):\n",
    "    \"\"\" Return a list of clean job titles \"\"\"\n",
    "    def preprocess_job_title(raw_job_title):\n",
    "        \"\"\" Clean a single job title\"\"\"\n",
    "        job_title = re.sub(r'\\(.*\\)', '', raw_job_title)  # Remove everything between parenthesis\n",
    "        return job_title.lower().strip()\n",
    "    return [preprocess_job_title(jt) for jt in job_titles]\n",
    "    \n",
    "pipeline = make_pipeline(\n",
    "    FunctionTransformer(preprocess_job_titles, validate=False),  # Preprocess the text\n",
    "    TextsToSequences(num_words=vocab_size),  # Turn word sequences into indexes sequences\n",
    "    Padder(max_length=max_words_job_title),  # Pad shorter job titles with a dummy index\n",
    ")\n",
    "\n",
    "# Note that the preprocessing pipeline must be fit on both the right and left examples\n",
    "# simultaneously\n",
    "pipeline.fit(list(dataset['job_left']) + list(dataset['job_right']));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_left = pipeline.transform(dataset['job_left'])\n",
    "x_right = pipeline.transform(dataset['job_right'])\n",
    "x_pairs = [x_left, x_right]   # this will be the input of the siamese network\n",
    "\n",
    "y_pairs = dataset['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-use the same embedding as with the baseline model\n",
    "embedding_layer = embedding.model.get_keras_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the pairs dataset and preprocessed the job titles we can turn our attention to the siamese model itself. It consists of a cloned sequential network, the input of which is a pair of vectors `x_left` and `x_right`. The last layer of the sequential network for `x_left` is the vector representation of the left job title and same thing for `x_right` the right input job title. The representations of the right and left inputs are used to compute the similarity between the job titles:\n",
    "\n",
    "$$\\text{sim}\\left(x_{l}, x_{r}\\right) = \\exp \\left(-\\| f(x_l) - f(x_r) \\|_1\\right),$$\n",
    "\n",
    "where $\\text{sim} \\in [0, 1]$, $\\|\\cdot\\|_1$ is the L1 norm, and $f$ is the function corresponding to the application of the cloned sequential network to the left/right input. \n",
    "\n",
    "This setting is called the [Manhattan LSTM](https://www.quora.com/What-is-Manhattan-LSTM) because we'll use LSTMs as the sequential network, and the L1 norm (used to compute the distance between two samples of a pair) is also called the [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry). Here is the corresponding code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "left_input (InputLayer)         (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_input (InputLayer)        (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_network (Sequential) (None, 64)           29852698    left_input[0][0]                 \n",
      "                                                                 right_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pair_representations_difference (None, 64)           0           sequential_network[1][0]         \n",
      "                                                                 sequential_network[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "masltsm_distance (Lambda)       (None, 1)            0           pair_representations_difference[0\n",
      "==================================================================================================\n",
      "Total params: 29,852,698\n",
      "Trainable params: 14,848\n",
      "Non-trainable params: 29,837,850\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, Subtract\n",
    "from keras import backend as K\n",
    "\n",
    "def exponent_neg_manhattan_distance(arms_difference):\n",
    "    \"\"\" Compute the exponent of the opposite of the L1 norm of a vector, to get the left/right inputs\n",
    "    similarity from the inputs differences. This function is used to turned the unbounded\n",
    "    L1 distance to a similarity measure between 0 and 1\"\"\"\n",
    "    return K.exp(-K.sum(K.abs(arms_difference), axis=1, keepdims=True))\n",
    "\n",
    "def siamese_lstm(max_length, embedding_layer):\n",
    "    \"\"\" Define, compile and return a siamese LSTM model \"\"\"\n",
    "    input_shape = (max_length,)\n",
    "    left_input = Input(input_shape, name='left_input')\n",
    "    right_input = Input(input_shape, name='right_input')\n",
    "\n",
    "    # Define a single sequential model for both arms.\n",
    "    # In this example I've chosen a simple bidirectional LSTM with no dropout\n",
    "    seq = Sequential(name='sequential_network')\n",
    "    seq.add(embedding_layer)\n",
    "    seq.add(Bidirectional(LSTM(32, dropout=0., recurrent_dropout=0.)))\n",
    "    \n",
    "    left_output = seq(left_input)\n",
    "    right_output = seq(right_input)\n",
    "\n",
    "    # Here we subtract the neuron values of the last layer from the left arm \n",
    "    # with the corresponding values from the right arm\n",
    "    subtracted = Subtract(name='pair_representations_difference')([left_output, right_output])\n",
    "    malstm_distance = Lambda(exponent_neg_manhattan_distance, \n",
    "                             name='masltsm_distance')(subtracted)\n",
    "\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=malstm_distance)\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "    return siamese_net\n",
    "\n",
    "siamese_lstm = siamese_lstm(max_words_job_title, embedding_layer)\n",
    "\n",
    "# Print a summary of the model mainly to know the number of trainable parameters\n",
    "siamese_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132636 samples, validate on 14738 samples\n",
      "Epoch 1/1\n",
      "132636/132636 [==============================] - 43s 321us/step - loss: 0.6732 - acc: 0.5754 - val_loss: 0.6548 - val_acc: 0.5948\n"
     ]
    }
   ],
   "source": [
    "siamese_lstm.fit(x_pairs, y_pairs, validation_split=0.1, epochs=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without much effort (light preprocessing, only one epoch, no early stopping, no hyper-parameters optimization) we obtain a decent ~60 % accuracy on the validation set. But remember that this is not the final task, here we are only solving the binary classification problem of recognizing pairs of job titles that belong to the same occupations category and pairs of jobs that are sampled from different occupations categories.\n",
    "\n",
    "To address the initial problem of finding each job title's category we have to compute, for each example in the test set, the similarity score of this example with all the examples in the training set. The predicted category is the one of the closest example in training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_references = pipeline.transform(x_train)  # Preprocess the training set examples\n",
    "\n",
    "def get_prediction(job_title):\n",
    "    \"\"\" Get the predicted job title category, and the most similar job title\n",
    "    in the train set. Note that this way of computing a prediction is highly \n",
    "    not optimal, but it'll be sufficient for us now. \"\"\"\n",
    "    x = pipeline.transform([job_title])\n",
    "    \n",
    "    # Compute similarities of the job title with all job titles in the train set\n",
    "    similarities = siamese_lstm.predict([[x[0]]*len(x_references), x_references])\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "    \n",
    "    # The predicted category is the one of the most similar example from the train set\n",
    "    prediction = train_set['SOC minor group'].iloc[most_similar_index]\n",
    "    most_similar_example = train_set['Reported Job Title'].iloc[most_similar_index]\n",
    "    return prediction, most_similar_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a prediction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled test job title: brand inspector\n",
      "True occupation: Agricultural Workers\n",
      "Occupation prediction: Agricultural Workers\n",
      "Most similar example in train set: deputy brand inspector\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 1\n",
    "pred, most_sim = get_prediction(x_test[sample_idx])\n",
    "\n",
    "print(f'Sampled test job title: {x_test[sample_idx]}')\n",
    "print(f'True occupation: {test_set[\"SOC minor group\"].iloc[sample_idx]}')\n",
    "print(f'Occupation prediction: {pred}')\n",
    "print(f'Most similar example in train set: {most_sim}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (siamese model): 39.36 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = [get_prediction(job_title)[0] for job_title in test_set['Reported Job Title']]\n",
    "accuracy = accuracy_score(classes_encoder.transform(y_pred), y_test)\n",
    "\n",
    "print(f'Test accuracy (siamese model): {100*accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The siamese model thus outperforms the random guess (accuracy ~0.1 %) and the nearest neighbor baseline (~25 %) by a substantial margin ü§π‚Äç‚ôÄÔ∏è . Even though it is far from perfect, it predicts the right category for a job title roughly 2 times out of 5, while it has to choose between roughly a hundred of them. \n",
    "\n",
    "What this means is that the siamese model managed to squeeze some juice out of **all** the examples in the dataset, even across different categories. The original multi-class classification approach does not allow to learn \"across categories\" because the categorical cross-entropy that is always used to treat those problems actually treats the multi-class classification tasks as a set of independent binary classification tasks. \n",
    "\n",
    "The siamese network approach to the few-shot learning problem is definitely a \n",
    "way out with textual data üìö. At the cost of a more complex modelling, it gives better results than standard  classification methods. Give it a shot if you want to classify your data with machine learning but don't have many examples per category, you won't regret it üéä. \n",
    "\n",
    "Please don't hesitate to leave a comment, I'd be happy to learn about your experiments with few-shot learning problems, to clarify some parts, or to have any feedback on this post. Thanks for reading! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
