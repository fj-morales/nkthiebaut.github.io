<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Data4thought: data science blog - Nicolas Thiebaut</title><link>https://nkthiebaut.github.io/</link><description>Nicolas Thiebaut's data science blog</description><lastBuildDate>Sat, 29 Feb 2020 02:00:00 +0100</lastBuildDate><item><title>Understanding the SHAP interpretation method: Kernel SHAP</title><link>https://nkthiebaut.github.io/kernel_shap.html</link><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;SHAP is certainly one of the most important tools in the interpretable machine learning toolbox nowadays. It is used by a variety of actors, mentioned extensively by the research community, and in my experience it provides the best insights into a model behavior.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Shap schema (from https://github.com/slundberg/shap)" class="center" src="images/shap_schema.png" width="50%" /&gt;&lt;/p&gt;
&lt;p&gt;This blog article gives a detailed yet simple explanation for Kernel SHAP, the core of the SHAP reactor.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nicolas Thiebaut</dc:creator><pubDate>Sat, 29 Feb 2020 02:00:00 +0100</pubDate><guid isPermaLink="false">tag:nkthiebaut.github.io,2020-02-29:/kernel_shap.html</guid><category>posts</category><category>python</category><category>machine learning</category><category>interpretable machine learning</category></item><item><title>Understanding the SHAP interpretation method: Shapley values</title><link>https://nkthiebaut.github.io/shapley.html</link><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Explainable artificial intelligence (XAI, a.k.a interpretable machine learning) is a thing those days. The goal of XAI is to provide explanations for machine learning models predictions, such that humans can understand the reasons that lead to those predictions.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Interpretable machine learning Google scholar searches over the last few years." class="center" src="images/xai-trend.png" width="40%" /&gt;&lt;/p&gt;
&lt;p&gt;It is important to know the reasons behind an algorithm's predictions in a variety of contexts:&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nicolas Thiebaut</dc:creator><pubDate>Fri, 27 Dec 2019 16:00:00 +0100</pubDate><guid isPermaLink="false">tag:nkthiebaut.github.io,2019-12-27:/shapley.html</guid><category>posts</category><category>python</category><category>machine learning</category><category>interpretable machine learning</category></item><item><title>Few-shot learning in NLP: many-classes classification from few examples</title><link>https://nkthiebaut.github.io/fewshot_learning_nlp.html</link><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;If you're doing machine learning and meet a classification problem with many categories and only a few examples per category, it is usually thought that you're in trouble üò®. Acquiring new data to solve this issue is not always easy or even doable. Luckily, we'll see that efficient techniques exist to deal with this situation with Siamese Neural Networks üï∫.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nicolas Thiebaut</dc:creator><pubDate>Sun, 19 Aug 2018 22:00:00 +0200</pubDate><guid isPermaLink="false">tag:nkthiebaut.github.io,2018-08-19:/fewshot_learning_nlp.html</guid><category>posts</category><category>python</category><category>machine learning</category><category>deep learning</category><category>natural language processing</category></item><item><title>LIME of words: interpreting Recurrent Neural Networks predictions</title><link>https://nkthiebaut.github.io/deep-lime.html</link><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This is the second part of my blog post on the LIME interpretation model. For a reminder of what LIME is and its purpose, please read the &lt;a href="https://nkthiebaut.github.io/lime-of-words.html"&gt;first part&lt;/a&gt;. This second part is a quick application of the same algorithm to a deep learning (LSTM) model, while the first part was focused on explaining the predictions of a random forest.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nicolas Thiebaut</dc:creator><pubDate>Tue, 12 Sep 2017 16:00:00 +0200</pubDate><guid isPermaLink="false">tag:nkthiebaut.github.io,2017-09-12:/deep-lime.html</guid><category>posts</category><category>python</category><category>machine learning</category><category>deep learning</category></item><item><title>LIME of words: how to interpret your machine learning model predictions</title><link>https://nkthiebaut.github.io/lime-of-words.html</link><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this blog post I will share experiments on the LIME (Local Interpretable Model-agnostic Explanations) interpretation model. LIME was introduced in 2016 by Marco Ribeiro and his collaborators in a paper called &lt;a href="https://arxiv.org/abs/1602.04938"&gt;‚ÄúWhy Should I Trust You?‚Äù Explaining the Predictions of Any Classifier&lt;/a&gt;. The purpose of this method is to explain a model prediction for a specific sample in a human-interpretable way.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nicolas Thiebaut</dc:creator><pubDate>Mon, 07 Aug 2017 16:00:00 +0200</pubDate><guid isPermaLink="false">tag:nkthiebaut.github.io,2017-08-07:/lime-of-words.html</guid><category>posts</category><category>python</category><category>machine learning</category></item></channel></rss>